networks:
  esanum-network: # ✅ Ensure a shared network

configs:
  backend_env:
    file: ../.env.production

services:
  backend:
    image: esanum-task-backend:latest
    ports:
      - "3001:3001"
    networks:
      - esanum-network # ✅ Add backend to network
    configs:
      - source: backend_env
        target: /app/.env.production  # ✅ Mount it inside the container
    command: npm run prod:server -w server
    depends_on:
      - redis
    volumes:
      - uploads:/app/uploads
    deploy:
      mode: replicated
      replicas: 1  # ✅ Single instance of backend
      restart_policy:
        condition: any
        delay: 5s

  frontend:
    image: esanum-task-frontend:latest
    ports:
      - "4300:80"
    networks:
      - esanum-network # ✅ Add backend to network
    command: nginx -g 'daemon off;'
    depends_on:
      - backend
    deploy:
      mode: replicated
      replicas: 1  # ✅ Single frontend instance
      restart_policy:
        condition: any
        delay: 5s

  redis:
    image: redis:latest
    ports:
      - "6380:6380"
    networks:
      - esanum-network # ✅ Add backend to network
    command: ["redis-server", "--port", "6380"]
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: any  # ✅ Restart always
        delay: 5s

  worker:
    image: esanum-task-worker:latest
    environment:
      NODE_ENV: production
    configs:
      - source: backend_env
        target: /app/.env.production  # ✅ Mount it inside the container
    networks:
      - esanum-network # ✅ Add backend to network
    command: npm run prod:worker -w server
    depends_on:
      - backend
      - redis
    volumes:
      - uploads:/app/uploads
    deploy:
      mode: replicated
      replicas: 5  # ✅ Scale dynamically
      restart_policy:
        condition: any
        delay: 5s

volumes:
  uploads: